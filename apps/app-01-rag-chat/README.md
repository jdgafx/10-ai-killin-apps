# Multi-Model AI Chat Platform with RAG

A production-ready vector-powered chat application with Retrieval-Augmented Generation (RAG) using Anthropic Claude via Cloudflare Workers.

## Features

- ðŸ¤– **Real AI Integration**: Anthropic Claude 3.5 Sonnet via Cloudflare Workers
- ðŸ“š **RAG Implementation**: Vector-based document retrieval for context-aware responses
- ðŸ’¾ **Client-Side Vector Store**: LocalStorage-based vector database with cosine similarity
- ðŸ“„ **Document Upload**: Upload and index text documents for RAG queries
- ðŸŽ¯ **Source Citations**: See which documents were used to answer your questions
- ðŸŽ¨ **Modern UI**: Clean, responsive interface with Tailwind CSS
- âš¡ **Edge Computing**: Global deployment with zero cold starts

## Tech Stack

- React 18 + Vite
- Tailwind CSS
- Lucide React Icons
- LocalStorage Vector Store
- AI Providers Package (workspace dependency)

## Setup

1. Install dependencies:
```bash
npm install
```

2. Configure environment variables:
```bash
cp .env.example .env
# Edit .env with your API keys
```

3. Start development server:
```bash
npm run dev
```

## How It Works

### RAG Pipeline

1. **Document Ingestion**: Upload documents â†’ Split into chunks â†’ Generate embeddings â†’ Store in vector DB
2. **Query Processing**: User query â†’ Generate query embedding â†’ Retrieve top-K similar chunks
3. **Context-Augmented Generation**: Combine retrieved context + user query â†’ Send to AI provider
4. **Response with Sources**: Display AI response + show source documents with similarity scores

### Vector Store

The application uses a client-side vector store implementation:
- Embeddings generated using text hashing (production should use real embedding APIs)
- Cosine similarity for semantic search
- LocalStorage persistence
- Support for metadata and chunking

## API Endpoint

**POST /api/chat**

Real Anthropic Claude API integration via Cloudflare Workers.

**Request:**
```json
{
  "message": "Your question here",
  "context": "Retrieved document context (optional)"
}
```

**Response:**
```json
{
  "content": "AI response text",
  "model": "claude-3-5-sonnet-20241022",
  "usage": {
    "input_tokens": 150,
    "output_tokens": 200
  }
}
```

**Environment Variables (Cloudflare):**
- `ANTHROPIC_API_KEY`: Your Anthropic API key

## Deployment

### Cloudflare Pages (Recommended)

**Using Wrangler CLI:**
```bash
npm run build
wrangler pages deploy dist --project-name=app-01-rag-chat
```

**Environment Setup:**
```bash
# Set secrets in Cloudflare dashboard or via CLI
wrangler pages secret put ANTHROPIC_API_KEY
```

**See full guide:** `/CLOUDFLARE_DEVELOPER_GUIDE.md`

### Alternative Deployments

**Vercel:**
```bash
vercel deploy
```

**GitHub Pages:**
```bash
npm run build
# Deploy dist/ folder to GitHub Pages
```

## Project Structure

```
src/
â”œâ”€â”€ App.jsx                    # Main application component
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ChatMessage.jsx       # Message display component
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ chat.js              # Chat logic with provider switching
â”‚   â””â”€â”€ rag.js               # RAG implementation (vector store, embeddings)
â””â”€â”€ main.jsx                 # Entry point
```

## Usage

1. **Upload Documents**: Click "Upload Document" to add context
2. **Select Provider**: Choose between MiniMax, Gemini, or DeepSeek
3. **Ask Questions**: Type your question and get RAG-powered responses
4. **View Sources**: See which documents were used with relevance scores

## Future Enhancements

- [ ] Real embedding APIs (OpenAI, Cohere, etc.)
- [ ] Cloud vector database (Pinecone, Weaviate)
- [ ] Multi-modal support (images, PDFs)
- [ ] Conversation memory and export
- [ ] Advanced chunking strategies
- [ ] Streaming responses

## License

MIT
